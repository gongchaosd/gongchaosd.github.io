<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[情商网络]]></title>
      <url>/2017/12/07/%E6%83%85%E5%95%86%E7%BD%91%E7%BB%9C/</url>
      <content type="html"><![CDATA[<h1 id="情商网络"><a href="#情商网络" class="headerlink" title="情商网络"></a>情商网络</h1><p>提出一个基于情商和智商网络结合的新的网络结构。</p>
<h2 id="首先要搞清楚的关于情商的问题："><a href="#首先要搞清楚的关于情商的问题：" class="headerlink" title="首先要搞清楚的关于情商的问题："></a>首先要搞清楚的关于情商的问题：</h2><ul>
<li>情商是什么？</li>
<li>情商有什么作用？</li>
<li>情商能不能数学建模？</li>
</ul>
<h2 id="情商网络构造"><a href="#情商网络构造" class="headerlink" title="情商网络构造"></a>情商网络构造</h2><ul>
<li>基于情商的知识，改进神经元的构造</li>
<li>以新的神经元为基础，提出新的网络模型</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 学习 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[hellomyBlog]]></title>
      <url>/2017/12/06/hellomyBlog/</url>
      <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>At present, the evolvement trend of mobility and light weight of deep learning networks has become increasingly obvious. Deep learning networks are progressively being applied to smart terminals and embedded devices, such as currently popular smartphone portrait photography and auto autopilot. Compared with the cloud servers, these computing devices are more close to the usage scenarios of artificial intelligence, and able to satisfy the real-time processing needs. However, their computing performance and storage space are significantly limited. At the same time, deep learning networks, especially deep convolution neural networks, are moving toward greater capacity, greater scale and deeper levels. Generally, high-capacity deep neural networks are inevitably accompanied by massive amount of neurons and connections. Take VGG-16 on the ILSVRC-2012 dataset, which is one of state-of-the-art deep models, as an example, it has 19 weight layers, 144 million parameters and occupies 552M storage space.\citeRN169 The contradiction between the trend of mobility and larger scale has led to the rapid development of deep learning network compression technology.<br>CNN pruning has been studied by several researchers.\citeRN12 \citeRN16 \citeRN15 And there are several approaches to compress large scale networks. Denil et al.\citeRN13 found the redundancy of parameters in DNNs, and pointed out that some of these parameters are not necessary to spend resources to train. \citeRN8 stored parameters of CNNs with vector quantization, and reduced the memory consumption by establishing indexes. Han Song et al. \citeRN17 implemented weight sharing by quantizing the weights after a pruning process, which filters out the nonsignificant connections ahead of schedule. \citeRN10 further introduced structured pruning using evolutionary particle filter on deep convolutional neural networks. Chen et al. \citeRN3 employed hashing function to group connection weights into hash buckets for weight sharing.<br>In this work we propose to squeeze the pruned network with hashing trick. The proposed work first explores sparsity of FC and CONV layers by pruning the unimportant connections. Then the remained high-precision weights are partially compressed into 2 hash buckets that eventually converging into a common centroid vector to reduce the CNN’s memory footprint. In the end we retrain the surviving connections and centroids to counteract the loss in accuracy. In the weight initialization phase of retraining, the proposed work compared 3 candidate approaches including taking the maximum, minimum and average value in the hash buckets as the initial real weights. Experiments show that the xxxx method has the most excellent performance.<br>Several favorable properties of our scheme is especially remarkable: 1. The hashing function is much more low-cost compared with scalar quantization, which consumes additional memory space for storing index data and more computing resources especially for Index establishing algorithm（kmeans\citeRN8, for example; 2. Packet handling of hashing procedure and weight sharing in virtual matrix will largely relieve the computational pressure of the overall hash function operation; 3. The weight sharing approach using hashing trick cooperates well with pruning in that pruning the unimportant connections beforehand will decrease hash collision making the weight sharing more efficient.<br>The rest of the paper is organized as follows. Section 2 presents. Section 3 explains . Experimental results are provided in Section . Finally Section  concludes the work.</p>
]]></content>
      
        
    </entry>
    
  
  
    
  
</search>
